# Next Word Predictor using LSTM


## ğŸ“Œ Overview
Next Word Predictor is an **LSTM-based** deep learning model that predicts the next word based on an input sequence. It utilizes **Natural Language Processing (NLP) techniques**, including tokenization, word embedding, and sequence modeling, to generate accurate predictions.

## ğŸš€ Features
âœ… Uses **LSTM neural networks** for next-word prediction  
âœ… Tokenizes and preprocesses text using **TensorFlow/Keras Tokenizer**  
âœ… Implements **sequence modeling** to learn word relationships  
âœ… Provides **real-time word prediction** based on input text  

---

## ğŸ”§ How to Run?
### ğŸ“Œ STEPS:
### Step 01: Clone the Repository
```bash
git clone https://github.com/your-username/next-word-predictor-lstm.git
cd next-word-predictor-lstm
```

### Step 02: Create a Conda Environment
```bash
conda create -n lstm_predictor python=3.8 -y
conda activate lstm_predictor
```

### Step 03: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 04: Train the Model
Run the training script to preprocess text and train the LSTM model:
```bash
python lstm_project.py
```

Alternatively, you can use the trained model for prediction:
```python
from lstm_project import tokenizer, model, pad_sequences
import numpy as np

def predict_next_word(text, num_words=1):
    for _ in range(num_words):
        token_text = tokenizer.texts_to_sequences([text])[0]
        padded_token_text = pad_sequences([token_text], maxlen=73, padding='pre')
        pos = np.argmax(model.predict(padded_token_text))
        for word, index in tokenizer.word_index.items():
            if index == pos:
                text += " " + word
                break
    return text

input_text = "Kamlesh Kumar is"
predicted_text = predict_next_word(input_text, num_words=10)
print("Predicted Sentence:", predicted_text)
```

---

## ğŸ“‚ Dataset
- The dataset consists of textual data used for training the model.  
- Preprocessing includes **tokenization, padding, and vocabulary creation** using TensorFlow/Keras Tokenizer.  

## ğŸ— Model Architecture
ğŸ”¹ **Embedding Layer**: Converts words into dense vector representations  
ğŸ”¹ **LSTM Layers**: Captures sequential dependencies in the text  
ğŸ”¹ **Dense Layer**: Outputs probability distribution over the vocabulary  

---

## ğŸ›  Dependencies
- Python 3.x  
- TensorFlow/Keras  
- NumPy  
- Pandas  
- Matplotlib  

## ğŸ“Š Results
âœ”ï¸ The model achieves **accurate next-word predictions** based on training data  
âœ”ï¸ Performance improves with **larger datasets and hyperparameter tuning**  

---

## ğŸ”® Future Improvements
ğŸ“Œ Implement **Bidirectional LSTMs** for enhanced accuracy  
ğŸ“Œ Train on **larger and diverse datasets** for better generalization  
ğŸ“Œ Integrate with a **web-based application** for real-time usage  

---

## ğŸ¤ Contributing
Feel free to **fork** the repository, submit **pull requests**, and report issues!  

## ğŸ“œ License
ğŸ“ This project is licensed under the **MIT License**.

